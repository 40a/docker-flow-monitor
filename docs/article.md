We are rewriting our monolithic applications into microservices. We are putting them into containers and deploying them through one of the schedulers. We are marching into a glorious future. There's nothing that can stop us now. Except... We, as an industry, are not yet ready for microservices. One thing is to design our own services in a way that they are stateless, fault tolerant, scalable, and so on. Unless you just started a new project, chances are that you still did not reach that point and that there are quite a few legacy services floating around. However, for the sake of brevity and the urge to get to the point I'm trying to make, I will assume that all the services you're in control of are truly microservices. Does that mean that the whole system reached that nirvana state? Is deployment of a service (no matter who wrote it) fully independent from the rest of the system? Most likelly it isn't.

Let's say that you just finished the first release of your new service. Since you are practicing continuous deployment, that first release is actually the first commit to your code repository. Your CD tool of choise detected that change, and started the process. At the end of it, the service is deployed to production. I can see a grimm on your face. The expression of happiness that can be seen only after a child is born or a service is deployed to production for the first time. That smile should not be long lasting since deploying a service is only the beginning. It needs to be integrated with the rest of the system. The proxy needs to be reconfigured. Logs parser needs to be updated with the format of the logs produced by the new service. Monitoring system needs to become aware of the new service. Alerts need to be created that will send warning and error messages when the state of the service reaches certain thresholds. The whole system needs to adapt to the new service and act in a way that self-healing processes incorporate the new variables introduced with the commit we made a few moments ago.

How to we adapt the system so that it takes the new service into account? How do we make that service be the integral part of the system?

Unless you are writing everything yourself (in which case you must be Google), your system consists of a mixture of services written by you and services written and maintained by others. You probably use a third party proxy (hopefully that's [Docker Flow Proxy](TODO)). You might have choosen the ELK stack for centralized logging. How about monitoring? It could be Prometheus. No matter the choices you made, you are not in control of the architecture of the whole system. Heck, you're probably not even in control of all the services your wrote.

Most of the third party services are not designed to work in a highly dynamic cluster. When you deployed that first release of the service, you might have had to configure the proxy manually. You might have had to add a few parsing rules to your LogStash config. Your Prometheus targets had to be updated. New alerting rules had to be added. And so on, and so forth. Even if all those tasks are automated, the CD pipeline would have to become too big and the process would have to be too flaky. The reason is quite simple. Most third-party services were designed in an era when clusters were a collection of static servers. Only a handful of those were designed to work well with containers and even fewer were adapted to work with schedulers (e.g. Swarm, k8s, Mesos/Marathon).

One of the major limitations of those third-party services is their reliance on static configuration. Take Prometheus as an example which can be extended to many other services. Every time we want to add a new target, we need to modify its configuration file and reload it. That means that we have to store that configuration file in a network drive, have some templating mechanism which is updated with every new service and, potentially, with every update of an existing service. So, we would deploy our fancy new service, update the template that generates Prometheus config, create a new config and overwrite the one stored on the network drive, and reload Prometheus. All that could be avoided if Prometheus would be configurable through its API. Still, a more extensive (not to say better) API would remove the need to templates but would NOT eliminate the need for a network drive. Its configuration is its state and it has to be preserved. The main problem is that the state is duplicated. The service itself should contain all the info that describes it. Part of that description should be metrics it exposes. If the service has that info, why do we need to replicate it in Prometheus? Shouldn't it be Prometheus' job to detect a new service and fetch the info it needs from it?
